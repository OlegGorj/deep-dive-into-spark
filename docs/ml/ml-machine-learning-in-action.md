# Reading Book Machine Learning In Action

## Preface
### 机器学习的应用
简单的说，机器学习算法在信息检索、数据挖掘在内的多个领域都有着十分广泛的应用。

以下应用都包含着大量的机器学习算法：
* 搜索引擎
* 社交网络分析
* 推荐引擎
* 计算广告
* 电子商务

对于一个电子商务网站来说，需要用到机器学习：
* 自动删除不适当的发布信息
* 检测不正当交易
* 给出用户可能喜欢的商品
* 预测网站的流量

### 程序员为何要学机器学习
当经费有限时，谁更能得到工作，思考者还是执行者？很可能是执行者，但当应用需要更高要求的算法时，那么需要的人员就必须能够阅读论文，领会论文思路并通过代码实现，如此反复下去。

### 机器学习是什么？
简而言之，机器学习可以解释数据背后的真实含义。

### 数据挖掘十大算法
Top 10 Algorithms in Data Mining.

1. C4.5决策树
2. K-均值(K-mean)
3. 支持向量机(SVM)
4. Apriori
5. 最大期望算法(EM)
6. PageRank
7. AdaBoost
8. k-近邻(kNN)
9. 朴素贝叶斯(NB)
10. 分类回归树(CART)

### 如何选择合适的算法
必须考虑以下两个问题：
* 使用机器学习算法的目的是什么，即想要完成何种任务。是有监督的还是无监督的，是分类还是回归，等等。
* 需要分析或收集的数据是什么。特征值是离散的还是连续的，是否有缺失值，是否有异常值，

一般来说，发现最好算法是反复试错的迭代过程。

### 开发机器学习应用程序的步骤
收集数据
准备输入数据
分析输入数据
训练算法
测试算法
使用算法

## kNN分类
### 伪代码
```
对未知类别的数据集中的每个点，依次执行以下操作：
(1) 计算已知类别数据集中的点与当前点之间的距离
(2) 按距离递增次序排序
(3) 选取与当前点距离最小的k个点
(4) 确定前k个点所在类别的出现频率
(5) 返回前k个点出现频率最高的类别作为当前点的预测分类
```

### 要点
* 基于实例的学习
* 距离计算。常用的有欧氏距离
* 归一化处理
* 评估算法的正确率。使用已有数据的90%作为训练集，10%作为测试集。
* 缺点：无法给出数据的内在含义
* 使用场景：手写识别；约会网站约会对象的分类

## 决策树
### 图像表示
* 判断块(decision block)，分支(branch)
* 终止块(terminating block)

### 伪代码(使用递归方式构建树)
```
function createBranch(dataset):
  if (dataset中的所有实例属于同一分类):
    停止
  else
    寻找划分dataset的最好特征
    划分dataset，并创建分支节点
    for (每个分支节点及其对应的subset)
      调用createBranch(subset)
```

### 选择特征
选择用于划分的特征是关键。划分数据集的大原则是：将无序的数据变得更加有序。使用信息论(information theory)中香农熵(entropy)可以度量集合的无序程度，即信息量的大小。量化划分数据集之前和之后信息量，所发生的变化，称为信息增益(information gain)，。选择信息增益最高的特征来进行划分。
```
li(x) = -log(pi(x))
```
Note: pi(x)是x选择分类i的概率, pi(x)=分类i的实例数目/总的实例数目。该等式表示分类i的信息量。
```
H = -Sigma(pi(x)log(pi(x)))
     i=[1,n]
```
Note: n是分类的数目。该等式表示整个数据集的信息量。

从上面的等式可以看出，数据集中实例所对应的类别越多，熵越高，越无序。

### 要点
* 度量集合无序程度的另一个办法是基尼不纯度(Gini impurity)。
* ID3是基本的决策树，用于划分标称型数据集，数值型数据需要离散化。更强大也更流行的决策树算法是C4.5和CART。
* 实现上，可以用python的dictionary表示树的节点，数据结构为：`{用于分支的属性名称:{属性值1:子树1,属性值2:子树2,...}}`
* 使用Matplotlib annotations来绘制树形图
* 使用python的pickle模块序列化决策树
* 应用场景：动物类型推断；隐形眼镜类型诊断
* 适用于沿着轴向容易划分的数据集。例如，对于二维空间，数据集容易用平行于x轴或y轴的直线进行划分。对于符合概率分布的数据集效果不会太好。

## 基于概率论的朴素贝叶斯分类
### 基本思路
朴素贝叶斯是贝叶斯决策理论的一部分。贝叶斯决策理论的核心思想：选择具有最高概率的决策。即：x为待分类的实例，存在多个分类ci(i = [1,n])，对x进行分类等价于计算x属于ci的概率，然后取概率最大的一个类别。关键是计算条件概率p(ci|x)。一种方法是直接计算：
```
p(ci|x) = p(ci and x)/p(x)
```
另一种方法是使用贝叶斯准则，交换条件概率中的条件与结果，通过已知的三个概率值来计算未知的概率值。
```
p(ci|x) = p(x|ci) * p(ci) / p(x) 
```

x是特征向量，朴素贝叶斯假设特征之间是互相独立的，这个假设正是朴素贝叶斯分类器中朴素(naive)一词的含义。朴素贝叶斯分类器的另一个假设是，每个特征同等重要。尽管上述假设都存在一些问题，但朴素贝叶斯的实际效果却很好。

如果x = (x0,x1,...)，并且x0,x1,...互相独立，则有：
```
p(ci|x)
= p(x|ci) * p(ci) / p(x)
= p((x0,x1,...)|ci) * p(ci) / p(x)
= p(x0|ci) * p(x1|ci) * p(ci) / p(x)
```
其中p(ci)表示ci类的实例个数占所有实例个数的比率。p(x0|ci)表示分类为ci的实例中，0号特征取值x0的实例的比率。

### 使用朴素贝叶斯进行文档分类
将文档向量化：将文档表示为token的向量，其中值1表示token出现在文档中，0表示token未出现。

### 要点
* 朴素贝叶斯分类时，某个概率为0，则最后的乘积为0，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2.另一个问题是下溢，由于太多很小的数相乘造成的，一种解决办法是对乘积取自然对数。在代数中有ln(a*b)=ln(a)+ln(b)，概率乘积变成对数相加。
* 除了将文档表示为词集模型(set-of-words model)，另一种方法是词袋模型(bag-of-words model)
* 最著名的一个应用是电子邮件的垃圾邮件过滤。另一个应用是过滤网站的恶意留言。
* hold-out cross validation(保留交叉验证)
* python的feedparser模块可用于获得RSS源

## Logistic回归
### 思路
对于二分类，输出为0或1.sigmoid函数有类似阶跃函数的性质，且数学上更易处理。
```
sigmoid(z) = 1 / (1 + e^-z)
```
为了实现logistic回归分类器，在每个特征上乘以一个回归系数，然后将所有结果相加，
```
z = w0x0 + w1x1 + ... + wnxn    令x0=1
```
将这个总和代入sigmoid函数中，进而得到一个0~1之间的值，大于0.5归入1类，小于0.5则归入0类。那么，最佳回归系数如何求得呢？目标是求得wi使得实际分类值和预测分类值之间的误差最小，这可以使用优化算法。

### 梯度上升法/梯度下降法
梯度上升法基于的思想是：要找到某函数的最大值，最好的办法是沿着函数的梯度方向探寻。求最大值用梯度上升法，求最小值则用梯度下降法。f(x1,x2,...,xn)的梯度，是f(x1,x2,...,xn)沿着各个分量方向的偏微分。所以，回归系数向量计算方法为：
```
权重向量w := w + alpha * f(w)的梯度    梯度上升法
权重向量w := w - alpha * f(w)的梯度    梯度下降法
```
梯度决定了移动方向，而alpha则决定了移动量的大小。

### 基本梯度上升法的伪代码
```
每个回归系数初始化为1
重复R次：
  计算整个数据集的梯度
  使用alpha * gradient更新回归系数的向量
  返回回归系数
```

### 画出决策边界
为了画出决策边界，设定0 = w0 + w1x1 + ... + wnxn，代入w值求出xi的关系式，即为最佳分隔面。

### 随机梯度上升
梯度上升法在每次更新回归系数时都需要遍历整个数据集，该方法处理100个左右的数据时尚可，但如果要处理数十亿样本和成千上万的特征，则计算复杂度太高。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升法是一个在线学习算法。与在线学习相对应，一次处理所有数据被称为是批处理。

除了一次处理一个样本点，还可以进一步改进：
* 在整个数据集上迭代多次
* 样本随机选择
* alpha动态减少

### 处理数据中的缺失值
可选做法是：
* 使用可用特征的均值来填补缺失值
* 使用特殊值来填补缺失值，如-1或0
* 忽略有缺失值的样本
* 使用相似样本的均值填补缺失值
* 使用其他机器学习算法预测缺失值

各种方案各有优缺点，采用哪种方案取决于实际应用中的需求。一般来说，如果类别标签缺失，应丢弃。给缺失值设置的特殊值不应该影响回归系数的更新。

## 支持向量机
支持向量就是离分隔超平面(separating hyperplane)最近的那些点，接下来要试着最大化支持向量到分隔面的距离。该问题是一个带约束条件的优化问题，分隔超平面写成w^T*x+b，求最优化的权重w和b。引入拉格朗日乘数法，可以将问题转化为关于alpha的两个式子，一个是要最小化的目标函数，一个是在优化过程中必须遵守的约束条件，于是问题转换成求一系列最优化的alpha。求得alpha之后，可以计算出w和b，从而得到分隔超平面。这是一个二次优化问题。

传统上使用二次规划求解方法来解决该问题，复杂并且低效。

Platt的SMO算法将大优化问题分解为多个小优化问题来求解。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。在结果完全相同的同时，SMO算法的求解时间短得多。SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，就增大其中一个同时减小另一个。即该算法通过每次只优化2个alpha值来加快SVM的训练速度。

核函数把某个特征空间映射到另一个特征空间。在通常情况下，这种映射会将低维特征空间映射到高维空间。可以将低维空间中的非线性问题转化为高维空间中的线性问题。径向基函数(radial basis function)是最流行的核函数(kernel)，是一个常用的度量两个向量距离的核函数。核方法不止在SVM中适用，还可以用于其他算法中。

支持向量的数目存在一个最优值。如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这也就是kNN。

## 回归(Regression)
回归与分类不同之处在于，目标变量是连续数值型。说到回归，一般是指线性回归(linear regression)。回归方程(regression equation)中的权重wi，称为回归系数(regression weights)。

### 标准线性回归
求平方误差的最小值。平方误差公式为：
```
  m
Sigma(yi-xi^T*w)^2
 i=1
```
用矩阵也可以写作`(y-Xw)^T*(y-Xw)`。对w求导，得到X^T*(Y-Xw)，令其等于0，解出
```
w = (X^T*X)^-1 * X^T * y
```
可见需要对矩阵求逆。该方法也称为ordinary least squares(普通最小二乘法)

### 局部加权线性回归(Locally Weighted Linear Regression, LWLR)
标准线性回归的一个问题是容易出现欠拟合。为了更好地拟合数据，采用局部平滑技术。LWLR中，我们给待预测点附近的每一个样本点赋予一定的权重，然后在这个子集上基于最小均方差来进行普通的回归。这种算法每次预测均需要事先选取出对应的数据子集。

权重的计算公式为：
```
w = (X^T * W * X)^-1 * X^T * W * y
```
其中W是一个矩阵，给每个样本点赋予权重。因为样本点越近，预测点越可能与之符合同一个线性模型，所以LWLR往往使用核(与支持向量机中的核类似)来给附近的点赋予更高的权重。核的类型可以自由选择，最常用的核是高斯核，对应权重如下：
```
w(i,i) = exp(|x(i)-x|/(-2 * k^2))
```
这样就构建了一个只含对角元素的权重矩阵W，并且预测点x与x(i)越近，w(i,i)将会越大。另外，k越小，权重衰减越快，预测点x所用的有效的局部样本区间越窄。

LWLR能更好的拟合，但是增加了计算量，因为它对每个点做预测时都必须使用整个数据集，为了做出预测，必须保留所有的训练数据。

使用较小的核，将得到较低的训练误差，但会造成过拟合，对新数据不一定能达到好的预测效果。在训练集上的误差的大小，与测试集上的误差是两回事，要选取到最佳模型，必须在未知数据上有好的效果。

### 系数缩减技术(shrinkage)
如果特征比样本点还多，则说输入数据的矩阵X不是满秩矩阵，非满秩矩阵在求逆时会出错。为了解决这个问题，统计学家引入了岭回归(ridge regression)。一个与岭回归有类似思路的缩减方法是lasso，lasso效果很好，但是计算复杂。另一类缩减方法称为前向逐步回归，可以得到与lasso差不多的效果，且更容易实现。

#### 岭回归
简单的说，岭回归就是在矩阵(X^T * X)上加上一个(lambda * I)，从而使得矩阵非奇异，进而能对(X^T * X)+(lambda * I)求逆，这里的I是单位阵。这样，回归系数的计算公式将变成：
```
w = ((X^T * X)+(lambda * I))^-1 * X^T * y
```
单位阵I，在0构成的平面上有一条1组成的岭，这就是岭回归中岭的由来。

缩减方法可以去掉不重要的系数，因此能够更好的理解数据。此外，与简单的线性回归相比，缩减方法能够取得更好的预测效果。

为了使用岭回归和缩减技术，需要对特征做标准化处理，使得每维特征具有相同的重要性。具体做法是所有特征都减去各自的均值再除以标准差。

#### lasso
还有其他一些缩减方法，如lasso，LAR，PCA回归以及子集选择等。与岭回归一样，这些方法不仅可以提高预测精确度，而且可以解释回归系数。

不难证明，在增加如下约束时，普通的最小二乘回归会得到与岭回归一样的公式:
```
  n
Sigma(wk^2) <= lambda
 k=1
```
与岭回归类似，lasso也对回归系数做了限定，对应约束条件如下:
```
  n
Sigma|wk| <= lambda
 k=1
```
这个条件大大增加了计算复杂度，需要使用二次规划算法。

#### 前向逐步回归
前向逐步回归可以得到与lasso差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有的权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。

伪代码如下：
```
数据标准化，使其分布满足0均值和单位方差
在每轮迭代过程中：
  设置当前最小误差lowestError为正无穷
  对每个特征：
    增大和缩小对应的系数：
	  改变系数得到一个新的W
	  计算新W下的误差
	  如果误差error小于当前最小误差lowestError，则设置Wbest等于当前W
	将W设置为新的Wbest
```
贪心算法在所有特征上运行两次for循环，分别计算增加或减少该特征对误差的影响。这里使用的是平方误差，通过之前的函数rssError得到。该误差初始值设为正无穷，经过与所有的误差比较后取最小的误差。整个过程循环迭代进行。

逐步线性回归的实际好处并不在于能绘出漂亮的图，主要的优点在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。

当使用缩减方法(如逐步线性回归或岭回归)时，模型增加了偏差(bias)，与此同时却减小了模型的方差(variance)。解释：如果模型与训练数据之间偏差过小，将产生过拟合，反而在实际数据上产生较大的方差；而如果模型与训练数据之间的偏差过大，则产生欠拟合，实际数据上的方差也会较大；在模型中引入适当的偏差，则有可能得到较小的方差。这种偏差与方差折中的概念，在机器学习中十分流行并且反复出现。

## 树回归
线性回归模型需要拟合所有的样本(局部加权线性回归除外)。在实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任意数据。一种可行的方法是将数据集切分成很多份容易建模的数据，然后利用线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树结构和回归法就相当有用。分类回归树(Classification And Regression Trees, CART)既可以用于分类又可以用于回归，非常值得学习。为了防止树的过拟合，采用树剪枝技术。模型树与回归树的做法不同，回归树在每个叶节点上使用各自的均值做预测，模型树算法要在每个叶节点上都构建出一个线性模型。

### CART算法
决策树是一种贪心算法，它要在给定时间内做出最佳选择，但并不关心能否达到全局最优。ID3算法切分过于迅速，且不能直接处理连续性特征。相比ID3算法，CART算法使用二元切分来处理连续型变量。

函数createTree如下：
```
找到最佳的待切分特征：
  如果该节点不能再分，将该节点存为叶节点
  执行二元切分
  在右子树调用createTree()方法
  在左子树调用createTree()方法
```

### 回归树(regression tree)
回归树和分类树的思路类似，但叶节点的数据类型不是离散型，而是连续型。

如何实现连续数据的切分呢？度量连续性数值的无序性(disorder)。总方差(平方误差的总值, total squared error)，可以通过均方差(mean squared error, variance)乘以样本点个数来得到。

chooseBestSplit函数的伪代码：
```
对每个特征：
  对每个特征值：
    将数据集切分成两份
	计算切分的误差
	如果当前误差小于当前最小误差，则将当前切分设定为最佳切分并更新最小误差
返回最佳切分的特征和阈值
```
chooseBestSplit有2个参数：误差下降值的下限，切分的最少样本数。最佳切分也就是使得切分后能达到最低误差的切分，如果切分数据集后效果提升不够大，那么就不应该进行切分操作而直接创建叶节点。另外，还需要检查两个切分后的子集大小，如果某个子集的大小小于用户设定的参数，那么也不应切分。最后，如果这些终止条件都不满足，那么就返回切分特征和特征值。

当chooseBestSplit函数确定不再对数据进行切分时，就可以创建叶节点的模型。在回归树中，该模型就是目标变量的均值。如果是模型树，其模型就是一个线性方程。

### 树剪枝(pruning)
一棵树如果节点过多，表明该模型可能对数据进行了过拟合。通过降低决策树的复杂度来避免过拟合的过程成为剪枝(pruning)。一种是预剪枝(prepruning)，函数chooseBestSplit中的终止条件，实际上就是在进行预剪枝；另一种形式的剪枝需要使用测试集和训练集，称作后剪枝(postpruning)。后剪枝利用测试集来对树进行剪枝，由于不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。

使用后剪枝方法需要将数据分成测试集和训练集。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差，如果是就合并。

### 模型树(model tree)
模型树的每个叶节点包含一个线性方程，分段线性函数(piecewise linear)。决策树相比于其他机器学习算法的优势之一在于结果更易理解。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。

可以利用上面的树生成算法对数据进行切分，且每份切分数据都能很容易被线性模型所表示，这里的关键在于误差的计算。对于给定的数据集，先用线性模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值，最后将这些差值的平方求和就得到了所需的误差。

### 讨论
模型树、回归树、分类树，哪一种模型更好呢？一个比较客观的方法是计算相关系数，也称为R^2值。可以调用NumPy的corrcoef(yHat, Y, rowvar=0)来求解，其中yHat为预测值，y是目标变量的实际值。

## K-均值
### 思路
k-均值是发现给定数据集的k个簇的算法。簇的个数k是用户给定的，每一个簇通过其质心(centroid)，即簇中所有点的中心来描述。

### 后处理以提高聚类性能
k-均值算法的一个缺点是有可能收敛到局部最优。一种度量聚类效果的指标是SSE(sum of squared error,误差平方和)，SSE越小表示数据点越接近于它们的质心，聚类效果越好。针对收敛到局部最小SSE的问题，在保持簇数目不变的前提下，如何提高簇的质量呢？可以对生成的簇进行后处理，对某些簇进行拆分，同时将某些簇进行合并。

拆分的方法。可以将具有最大SSE值得簇划分成两个簇。具体实现时可以将最大SSE簇包含的点过滤出来并在这些点上运行k-均值算法，其中k=2。

合并的方法。选择哪两个簇合并，有两种可以量化的办法：合并最近质心，或者合并两个使SSE增幅最小的质心。

### 二分k-均值
为了克服k-均值收敛于局部最优的问题，可以使用二分k-均值(bisecting k-means)。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分，取决于对其划分是否可以最大程度降低SSE，或者选择SSE最大的簇进行划分，直到簇数目达到用于指定的数目为止。

### 应用
* 对地图上的点进行聚类，距离度量采用球面余弦定理

## 关联规则学习
### 概念和术语
* 关联分析(association analysis)
* 关联规则学习(association rule learning)
* 物品组合空间巨大，蛮力(brute-force)搜索效率太低
* 频繁项集(frequent item set)
* 关联规则(association rule)
* 项集的支持度(support)，也就是覆盖量(coverage)：项集在所有数据集中出现的比例
* 关联规则的置信度(confidence)：`confidence(P -> H) = support(P | H)/support(P)`
* 前件(antecedent)
* 后件(consequent)

关联分析，从大规模数据集中寻找项之间的关系。这些关系有两种形式:频繁项集(Frequent Itemsets)和关联规则(Association Rules)。首先找到频繁项集，然后再从频繁项集中获得关联规则。

常用算法:
* Apriori
* FP-Growth

### Apriori核心思想
Apriori原理: 频繁项集的所有子集都是频繁的；反之，对一个非频繁项集而言，其超集也一定是非频繁的。

不管是发现频繁项集还是关联规则，算法的核心操作都是join和prune。

### Apriori算法描述(使用Apriori发现频繁项集)
#### 算法步骤
1. 生成所有单项项集C1
2. 去掉不满足最小支持度的项集得到L1
3. 剩下的项集，假设每个项集中元素个数为k，则使用上文提到的组合方法，生成包含项个数为k+1的项集集合 
4. 重复2和3，直至3无法组合更大的项集为止

#### Rationale
通过蛮力搜索的方式，从1项项集开始，逐步增加项集中的项的数目，可以考察所有可能的项集。这个过程中，可以利用上面的原理减少候选项集的个数。假设Ck是一系列候选的k项项集，逐一考察以过滤掉频度低于minSupport的项集，得到Lk，即一系列具有较大support的k项项集。为了继续考察k+1项项集，我们从Lk中选择k项项集合并为k+1项项集即可，合并方法为，Lk中选择前k-1项相同的两个项集，将前k-1项加上这两个项集各自独有的2个项，合并成候选的k+1项项集，这个集合记为C(k+1)。可以证明C(k+1)是L(k+1)的超集，即只要考察C(k+1)就可以得到所有频繁的k+1项项集，绝不会遗漏。因为C(k+1)可能会比所有的k+1项项集组成的集合小得多，算法效率也较高。为什么C(k+1)是L(k+1)的超集呢？只需要证明L(k+1)中的项集一定也在C(k+1)中。对于L(k+1)中任意一个频繁的k+1项项集{I1,I2,...,I(k-1),Ik,I(k+1)}，我们不妨将其拆分为两个频繁的k项项集，一个是{I1,I2,...,I(k-1),Ik}，另一个是{I1,I2,...,I(k-1),I(k+1)}，这两个项集都在Lk中，根据前面提到的C(k+1)的构造方法，Lk中的这两个项集一定会被合并到C(k+1)中，以此得证。

因此，可以从单项项集开始，通过组合较小的频繁项集形成更大的频繁项集，不必要对所有项集进行考察。

整个算法过程可以简记为：

    D -> C1 -> L1 -> C2 -> L2 -> ... -> Ck -> Lk -> C(k+1) -> ...

`Ck -> Lk`是一个过滤的处理，去掉频度低于minSupport的项集；`Lk -> C(k+1)`是一个合并的处理，合并方法见上文。

### Apriori算法描述(从频繁项集挖掘关联规则)
#### 算法步骤
1. 为一个频繁项集创建一个初始规则列表，其中规则后件只包含一个项
2. 对这些规则进行筛选，去掉所有不满足最小置信度的规则
3. 假设剩余规则后件包含k个项，按上文方法合并剩余规则来创建新的规则列表，其中规则后件包含k+1个元素
4. 重复2和3，直至3无法组合更大的后件为止

#### Rationale
对于一个给定的频繁项集I，我们可以把一部分项作为前件P(antecedent)，把另一部分作为后件H(consequent)，`I = P | H`，`|`表示集合的并操作。一条规则`P -> H`的置信度则定义为`support(P | H)/support(P)`，很容易看出来，后件H中的项越多，则P中的项就越少，support(P)就越大，因为support(P | H)=support(I)是固定的，所以`P -> H`的置信度也就越小。为给定的频繁项集挖掘大于一定置信度的关联规则时，从后件包含1项的规则开始，逐步增加后件中项的个数，置信度偏低的后件，其超集作为后件，对应的规则的置信度只会更低，所以可以不用考虑。跟上文一样，将符合条件的后件进行合并以得到更大的后件，考察这样的后件就足够了，合并方法跟上文一样。这样可以有效的减少需要考察的规则的数目。

对给定的频繁项集，挖掘关联规则的算法过程简记为:

    后件C1 -> 后件L1 -> 后件C2 -> 后件L2 -> ... -> 后件Ck -> 后件Lk -> 后件C(k+1) -> ...

### 应用
* 购物篮分析
* 搜索引擎的关联查询词推荐
* 网站上访问网页的相关性
* 发现Twitter中的共现词(co-occurring word)

## FP-Growth
## 基本概念
只需对数据集进行2次扫描，就能发现频繁项集，比Apriori高效，但是不能用来发现关联规则。

常用术语:

* FP Tree
* Item Header Table: 通过头指针表中的每个项所对应的链表，可以快速地找到该项在FP树上的所有节点。
* prefix path: FP树上的某个节点，从其父节点向根节点回溯，所构成的路径称为该项的前缀路径。
* Conditional Pattern Base：n的所有前缀路径构成的集合，也叫子模式基(sub-pattern base)
* Conditional FP Tree: 利用条件模式基所创建的FT树

### 思路
首先构建FP树，然后从FP树种挖掘频繁项集。

为了构建FP树，需要对原始数据集扫描两遍。第一遍对所有元素项的出现次数进行计数。根据Apriori原理，如果某元素项不是频繁的，则包含该元素的超集也是不频繁的，所以就不需要考虑这些超集。数据库的第一遍扫描统计出现的频率，而第二遍扫描中只考虑那些频繁元素。在第二次扫描数据集时会构建一颗FP树。构建FP树时，往FP树上不断添加项集时，树也随之成长，这也就是growth一词的来源。

### 算法描述(构建FP Tree)
#### 算法步骤
1. 遍历每个transaction，对其中的每个项，按其在总数据集中出现的次数排序，并去掉不满足最小支持度的项。
2. 对每个transaction，抽取出满足最小支持度的项，并按支持度从大到小顺序排列组成一条路径，将该路径从树的根节点依次往下插入，如果节点已存在，则递增节点的计数值，否则创建一个分支。
3. 对树中每一个新加入的节点，都要在头指针表中查找，如果头指针表中没有此项，就在头指针表中创建一个此项节点。然后，把新加入项接入到头指针表中此项所对应链表的末尾。
4. 重复2和3，直到所有transaction都处理完毕。

#### Rationale
header table实际上并不是必须的，只是为了提高FP树上的项的查找效率，以进一步提高FP-Growth的效率。

FP树本质上是一种紧凑的数据结构，将所有transaction信息都编码在这个紧凑的数据结构中。

* 所有的transaction都会在FP树中体现出来，即体现为一条从根节点开始的路径。
* 包含节点n的transaction = 包含n的路径
* FP树路径上的节点，其计数从根节点开始按降序排列

### 算法描述(从FP树挖掘频繁项集)
#### 算法步骤
对FP树进行挖掘，是一个递归算法，主要参数是FP树和后缀模式S。初始的FP树从原始数据集构建，suffix pattern为空。

对头指针表中的项按计数从小到大进行排序。然后对每一个频繁项I，执行如下步骤:

1. 头指针表中的该频繁项I，与递归传进来的后缀S合成频繁项集F并输出。
2. 得到该频繁项I的条件模式基。方法是，从该项在FP树中的节点开始向根节点回溯得到条件模式，条件模式的计数为该频繁项节点上的计数，所有的条件模式构成条件模式基。
3. 根据2中得到的条件模式基创建新的FP树。
4. 对新的FP树进行递归挖掘，将F作为新的S传入。

#### Rationale
不管是初始的FP树，还是递归过程中的FP树，构建FP树的过程，可以统一看作从pattern base构建FP tree的过程。初始的数据集，可以看作是初始的conditional pattern base，其condition为空，即suffix pattern为空，且每个pattern的计数为1。

挖掘FP树的过程，实际上可以抽象为，先对单个频繁项排序，然后进行采用分治法。例如对于I1 < I2 < I3 < ...，分别挖掘包含I1的频繁项集，挖掘不包含I1但是包含I2的频繁项集，挖掘不包含I1、I2但是包含I3的频繁项集，...，这个分类是完备的。对于每一个Ik，挖掘不包含I1到I(k-1)但是包含Ik的频繁项集，只需要FP树上Ik到树根的路径上的项，这些路径去掉Ik之后构成了Ik的条件模式基，如果能够得到Ik的条件模式基对应的频繁项集，再加上必须要包含的Ik，就正好挖掘出了不包含I1到I(k-1)但是包含Ik的频繁项集。

另外，单路径的FP树的挖掘，不需要递归，只要得到项的全部组合即可。

## 辅助工具
降维技术: 提炼数据中的重要信息并且移除噪声。

## 降维技术(Dimensionality Reduction)和PCA
大规模特征下，对数据进行简化，原因有：
* 更易显示
* 数据集更易使用
* 降低算法计算开销
* 去除噪声
* 使结果更易懂

数据降维技术作为预处理步骤，在数据应用到其他算法之前清洗数据。在已标记数据和未标记数据上都有降维技术。

常用的降维技术包括：
* 主成分分析(Principal Component Analysis, PCA)。PCA可以从数据中识别其主要特征，它是通过沿着数据最大方差方向旋转坐标来实现的。
* 因子分析(Factor Analysis)。假设观察数据是隐变量(latent variable)和噪声的线性组合，则如果隐变量的数目比观察数据特征数目少，则通过找隐变量能实现降维。
* 独立成分分析(Independent Component Analysis, ICA)。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，如果数据源少于观察数据特征数目，则可以降维。

其中PCA的应用是目前最广泛的。

### PCA
在PCA中，数据从原来的坐标系转换到新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴选择和第一个坐标轴正交且具有最大方差的方向，该过程一直重复，重复次数为原始数据中特征的数目，我们会发现，大部分方差都包含在最前面的几个新坐标轴中。因此，我们可以忽略余下的坐标轴，即对数据进行了降维处理。

伪代码如下：
```
去除平均值
计算协方差矩阵
计算协方差矩阵的特征值和特征向量
将特征值从大到小排序
保留最前面的N的特征向量
将数据转换到上述N个特征向量构建的新的空间中
```

特征值分析。如果`A*v=lambda*v`，则v称为矩阵A的特征向量，lambda称为A的特征值。如果找到矩阵A的k个特征向量，则
```
A*(v1,v2,...,vk)=(lambda1*v1, lambda2*v2,...,lambdak*vk)
```
从而将A线性变换为列数为k的矩阵。

## SVD(Singular Value Decomposition, 奇异值分解)
利用SVD能够用小得多的数据集来表示原始数据集。这样做，实际上是去除了噪声和冗余信息。

SVD最早是用于信息检索，称为隐语义索引(Latent Semantic Indexing, LSI)或隐语义分析(Latent Semantic Analysis)。在LSI中，一个矩阵由文档和词语组成当我们在该矩阵上应用SVD时，就会构建出多个奇异值，这些奇异值代表了文档的概念或主题。

SVD的另一个应用是推荐系统。先利用SVD从数据中构建一个主题空间，然后再在该空间下计算其相似度。

SVD是矩阵分解的一种类型，而矩阵分解是将数据矩阵分解为多个独立部分的过程。
```
Data(m*n) = U(m*m) Sigma(m*n) VT(n*n)
```
其中Sigma是对角阵，且对角元素从大到小排列。这些对角元素称为奇异值(singular value)。奇异值和特征值是有关系的。
```
奇异值 = Data*DataT特征值的平方根
```
一个普遍的事实是，在若干个奇异值后，其他的奇异值都是0。确定要保留的奇异值的数据有很多启发式的策略，其中一个典型做法是保留矩阵90%的能量信息，即将奇异值平方和累加到总值的90%为止，另一个启发式策略是如果有上万的奇异值，则保留前面的2000~3000个。

### 将SVD应用于推荐引擎
相似度计算
* 欧氏距离
* 皮尔逊相关系数。使用python的corrcoef()函数
* 余弦相似度。计算的是两个向量夹角的余弦值。cos(theta)=A*B / ||A||*||B||，其中||A||是向量A的范数(norm)，即向量的长度。

```
def estimateRatingFor(item_i):
  similairityTotal = 0
  ratingWeightedBySimilarityTotal = 0
  for item_j in all_other_items:
    calculate similarity between item_i and item_j
    similairityTotal += similarity
    ratingWeightedBySimilarityTotal += similarity * userRating_of_item_j
    return ratingWeightedBySimilarityTotal/similairityTotal
```

## Appendix
### Python
numpy

### 线性代数


### 概率论


### 资源
* 最有名的机器学习数据资源. http://archive.ics.uci.edu/ml
* 大数据爱好者不能错过，真正的大数据. http://aws.amazon.com/cn/publicdatasets/
* http://www.data.gov
* http://www.infochimps.com/
